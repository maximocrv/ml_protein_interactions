# Protein-Protein Interactions

The objective of this project is to reliably predict the relative binding affinity arising due to mutations in protein structures, which is an active field of research in protein engineering.

![protein_image](data/1a22_complex.jpeg)

## File Structure

* * * * * * * * * 

1. `data`: This directory contains all the data required for training and validation. The data is downloaded from the [SKEMPI v2.0 website](https://life.bsc.es/pid/skempi2/).

    - `pdbs_wt/`: This directory contains the PDB files corresponding to the wild type complexes.

    - `pdbs_mut/`: This directory contains the PDB files corresponding to mutants of the complexes in `pdbs_wt/`. The mutants are generated by applying random missense mutations to the SKEMPI complexes.

    - `skempi_v2_cleaned.csv`: This CSV file contains information about the PDB complexes in a more readable format.

    - `extracted_features.csv`: This CSV file contains the extracted feature matrices in a more readable format.

* * * * * * * * * 

2. `scripts`: This directory contains all the Python scripts used for data acquisition, preprocessing, training and validation. 

    - `constants.py`: It contains a list of all the common constant values used by rest of the scripts.

    - `pdb_pipeline.py`: It cleans and generates the features required for the models using OpenMM for simulation.

    - `preprocessing_mlp.py`: It preprocesses the features generated by `pdb_pipeline.py` and generates a CSV for usage by `mlp.py`.

    - `mlp.py`: It implements the MLP model approach.

    - `tune_xgboost.py`: It implements the XGBoost model approach.

    - `hydranet.py`: It implements the HydraNet model approach.

    - `utilities.py`: It contains simple utility functions which are used by rest of the scripts.

    - `train_helpers.py`: It contains the common functions used for training in the different models.

* * * * * * * * * 

3. `notebooks`: This directory contains all the Jupyten Notebooks used for exploring the dataset and potential machine learning parameters. 

    - `data_exploration.ipynb`: It is used for visualising and exploring the dataset for this project.
    - `model_exploration.ipynb`: It is used for data preprocessing targetted towards the MLP and XGBoost models.  

* * * * * * * * * 

## Requirements

* Anaconda or [Miniconda](https://docs.conda.io/en/latest/miniconda.html) (latter takes less time to install) as your python distribution.

> **Note**: due to the `pdbfixer` and `openmm` packages, this project is running on Python 3.7.

> **Note**: having an Nvidia GPU available is highly recommended, although not necessary.

## Installation

Install the env with the following command:
```
conda env create -f environment.yml
```

Now activate the conda environment by typing:
```
conda activate ml_protein_protein
```

If you need to update the environment use:
```
conda env update -f environment.yml
```

If for whatever reason you need to delete the environment due to conflicts, you can run:
```
conda env remove -n ml_protein_protein
```

Thereafter you have to run `conda deactivate` and you will be able to reinstall everything properly. 

> A `Makefile` already provides these commands for faster access. Simply run `make install-env`.

If you want to make sure your files are pep8 compliant before committing then create a pre-commit file in .git/hooks/
with the following content:
```
#!/bin/sh
set -e

flake8 --max-line-length=120
```
