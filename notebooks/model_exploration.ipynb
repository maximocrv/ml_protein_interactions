{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8a2ca59306ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_features_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_features_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/ml_protein_protein/scripts/train_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m def train(_model: torch.nn.Module, _criterion: torch.nn.Loss, dataset_train: DataLoader, dataset_test: DataLoader,\n\u001b[0m\u001b[1;32m     11\u001b[0m           _optimizer: torch.optim.Optimizer, n_epochs: int, device: torch.device, test_metrics: dict, log):\n\u001b[1;32m     12\u001b[0m     \"\"\"\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Loss'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from train_helpers import train, gen_loaders\n",
    "from constants import test_metrics, mlp_features_columns\n",
    "from utilities import load_data, open_log, clip_features_inplace, transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print('WARNING: using CPU.')\n",
    "    log.write('\\tWARNING: using CPU.\\n')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP and XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = X\n",
    "# plot_x = x_train\n",
    "# plot_x = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "N = len(mlp_features_columns)\n",
    "Nsqrt = int(np.sqrt(N)+0.5)\n",
    "for i in range(1, N-1):\n",
    "    fig.add_subplot(Nsqrt, Nsqrt, i)\n",
    "    plt.hist(plot_x[:, i-1])\n",
    "    plt.title(mlp_features_columns[i])\n",
    "fig.add_subplot(Nsqrt, Nsqrt, N-1)\n",
    "plt.hist(y)\n",
    "plt.title('$\\Delta\\Delta G$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('mlp_features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'min-max temperature: {np.min(X[:, -1]), np.max(X[:, -1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lennard Jones mean energies and standard deviations have a few outliers that skew the distribution.\n",
    "\n",
    "Here we show the 50 lowest and highest values of these features, showing that the values with different orders of magnitude are few,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_lj_mut_mean\n",
    "sulj = np.sort(X[:, 8])\n",
    "print(sulj[:50])\n",
    "print('------------------------------------------------------------------------')\n",
    "print(sulj[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_lj_mut_std\n",
    "sulj = np.sort(X[:, 9])\n",
    "print(sulj[:50])\n",
    "print('------------------------------------------------------------------------')\n",
    "print(sulj[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clip these features,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clip = np.copy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clip[:, 8] = np.clip(X[:, 8], 1e8, 5e9) # u_lj_mut_mean\n",
    "X_clip[:, 9] = np.clip(X[:, 9], 1e8, 5e10) # u_lj_mut_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X_clip, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "x_train, x_test = transform_data(x_train, x_test, degree=1, log=False, cross=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "N = len(mlp_features_columns)\n",
    "Nsqrt = int(np.sqrt(N)+0.5)\n",
    "for i in range(1, N-1):\n",
    "    fig.add_subplot(Nsqrt, Nsqrt, i)\n",
    "    plt.hist(x_train[:, i])\n",
    "    plt.title(mlp_features_columns[i])\n",
    "fig.add_subplot(Nsqrt, Nsqrt, N-1)\n",
    "plt.hist(y)\n",
    "plt.title('$\\Delta\\Delta G$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('mlp_features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_lj_mut_mean\n",
    "sulj = np.sort(X[:, 8])\n",
    "print(sulj[:50])\n",
    "print('------------------------------------------------------------------------')\n",
    "print(sulj[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_lj_mut_std\n",
    "sulj = np.sort(X[:, 9])\n",
    "print(sulj[:50])\n",
    "print('------------------------------------------------------------------------')\n",
    "print(sulj[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, layers, nodes, dropout=0.0, do_batchnorm=False, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_dim, nodes)\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.dropout = nn.ModuleList()\n",
    "        self.do_batchnorm = do_batchnorm\n",
    "        if do_batchnorm:\n",
    "            self.batchnorm = nn.ModuleList()\n",
    "        for _ in range(layers):\n",
    "            self.hidden.append(nn.Linear(nodes, nodes))\n",
    "            self.dropout.append(nn.Dropout(dropout))\n",
    "            if do_batchnorm:\n",
    "                self.batchnorm.append(nn.BatchNorm1d(nodes))\n",
    "        self.out = nn.Linear(nodes, output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        for i, layer in enumerate(self.hidden):\n",
    "            x = self.relu(layer(x))\n",
    "            if self.do_batchnorm:\n",
    "                x = self.dropout[i](self.batchnorm[i](x))\n",
    "            else:\n",
    "                x = self.dropout[i](x)\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(\n",
    "    input_dim=13,\n",
    "    layers=64,\n",
    "    nodes=60,\n",
    "    dropout=0.2,\n",
    "    do_batchnorm=False,\n",
    "    output_dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in model.hidden[0].parameters():\n",
    "    print(p)\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_W = 1e10\n",
    "max_W = -1e10\n",
    "min_b = 1e10\n",
    "max_b = -1e10\n",
    "for layer in model.hidden:\n",
    "    param_iter = layer.parameters()\n",
    "    W = next(param_iter).detach().numpy()\n",
    "    b = next(param_iter).detach().numpy()\n",
    "    if np.max(W) > max_W:\n",
    "        max_W = np.max(W)\n",
    "    if np.min(W) < min_W:\n",
    "        min_W = np.min(W)\n",
    "    if np.max(b) > max_b:\n",
    "        max_b = np.max(b)\n",
    "    if np.min(b) < min_b:\n",
    "        min_b = np.min(b)\n",
    "print(f'max weight: {max_W:12.5}    min weight: {min_W:12.5}')\n",
    "print(f'max bias:   {max_b:12.5}    min bias:   {min_b:12.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "# weights\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.hist(W.ravel() / W.size, bins=10)\n",
    "plt.title('Weights')\n",
    "\n",
    "# biases\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.hist(b / b.size, bins=10)\n",
    "plt.title('Biases')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('mlp_init_params.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we explore how different amounts of nodes per hidden layer lead to overfitting in the model. We perform an analysis on the layers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, dataset_train, dataset_test, optimizer,\n",
    "          n_epochs, device, test_metrics, best_heuristic=lambda x, y: True):\n",
    "    \"\"\"\n",
    "    Trains the given model according to a certain criterion and logging\n",
    "    the given test metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        model to be trained\n",
    "    criterion : torch.nn._Loss\n",
    "        criterion to use for training (for ex.: RMSE)\n",
    "    dataset_train : DataLoader\n",
    "        dataset used for training\n",
    "    dataset_test : DataLoader\n",
    "        dataset used for testing/validation\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        optimizer to use for training (for ex.: Adam)\n",
    "    n_epochs : int\n",
    "        max number of consecutive epochs allowed without achieving a\n",
    "        better evaluation score\n",
    "    device : torch.device\n",
    "        device to use for training (for ex.: CPU or CUDA)\n",
    "    test_metrics : dict\n",
    "        python dictionary with keys equal to the name of the test metric\n",
    "        to use and values equal functions of the form:\n",
    "        \n",
    "            fun(y_real, y_pred) -> float\n",
    "            \n",
    "    best_heuristic : lambda function\n",
    "        lambda function of the form:\n",
    "        \n",
    "            lambda x, y -> boolean\n",
    "        \n",
    "        The inputs are the overall best evaluation scores and \n",
    "        the current epoch's evaluation scores.\n",
    "        If the output is `True`, the current evaluation score\n",
    "        replaces the overall one. Otherwise, no change is\n",
    "        performed.\n",
    "        By default, this function always returns `True`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss : float\n",
    "        final train loss\n",
    "    eval_scores : dict\n",
    "        final test scores/losses\n",
    "    \"\"\"\n",
    "    print(\"Starting training\")\n",
    "    \n",
    "    best_eval  = None\n",
    "    best_epoch = 0\n",
    "    best_train = None\n",
    "    \n",
    "    best_counter = 0\n",
    "    epoch = -1\n",
    "    while best_counter < n_epochs:\n",
    "        best_counter += 1\n",
    "        epoch += 1\n",
    "        \n",
    "        # Train an epoch\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        # TODO: add batchnorm and dropout\n",
    "        for batch_x, batch_y in dataset_train:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            logits = model.forward(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Compute the gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model with a gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Test the quality on the test set\n",
    "        model.eval()\n",
    "        eval_scores = {key: [] for key in test_metrics}\n",
    "        n = 0\n",
    "        for batch_x, batch_y in dataset_test:\n",
    "            # to device\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            # TODO: solve allocation error in GPU\n",
    "            prediction = model(batch_x)\n",
    "\n",
    "            # back to host\n",
    "            prediction = prediction.cpu()\n",
    "\n",
    "            # fix tensors to be used with test metrics\n",
    "            # detach() excludes the tensor in the backprop caluclation\n",
    "            prediction = prediction.detach().numpy().ravel()\n",
    "            batch_y = batch_y.detach().numpy().ravel()\n",
    "\n",
    "            # weighted sum of scores according to batch_y length\n",
    "            for key, fun in test_metrics.items():\n",
    "                eval_scores[key].append(\n",
    "                    len(batch_y)*fun(batch_y, prediction)\n",
    "                )\n",
    "\n",
    "            n += len(batch_y)\n",
    "\n",
    "        eval_scores = {key: sum(eval_scores[key])/n for key in test_metrics}\n",
    "        \n",
    "        if best_eval is None or best_heuristic(best_eval, eval_scores):\n",
    "            best_eval  = eval_scores\n",
    "            best_epoch = epoch\n",
    "            best_train = train_loss\n",
    "            best_counter = 0\n",
    "        \n",
    "        if epoch%5==0:\n",
    "            eval_scores_str = \\\n",
    "                ' '.join([f'{k}={v:12.5g}' for k, v in eval_scores.items()])\n",
    "            print(\"Epoch {} | Train loss: {:12.5g} Test scores: {}\".format(\n",
    "                epoch, train_loss, eval_scores_str))\n",
    "    \n",
    "    \n",
    "    eval_scores_str = \\\n",
    "        ' '.join([f'{k}={v:12.5g}' for k, v in best_eval.items()])\n",
    "    print(\"BEST - Epoch {} | Train loss: {:12.5g} Test scores: {}\".format(\n",
    "        best_epoch, best_train, eval_scores_str))\n",
    "    return best_train, best_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these are the variables we will be storing in memory. Further down we will also store them in `pkl` format for ease reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'all_train_layers' in globals():\n",
    "    all_train_layers  = {}\n",
    "if not 'all_scores_layers' in globals():\n",
    "    all_scores_layers = {}\n",
    "if not 'all_train_nodes' in globals():\n",
    "    all_train_nodes   = {}\n",
    "if not 'all_scores_nodes' in globals():\n",
    "    all_scores_nodes  = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the main cross validation section. By default, it performs the analysis over the number of nodes, fixing the number of layers to `8`.\n",
    "\n",
    "To perform the analysis on the number of layers as well, this cell has to be re-run with:\n",
    "```python\n",
    "n_hidden_layers = [2, 4, 6, 8, 10]\n",
    "n_hidden_nodes = [128]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import PearsonRConstantInputWarning\n",
    "import time\n",
    "warnings.simplefilter(\"ignore\", PearsonRConstantInputWarning)\n",
    "\n",
    "# hyperparameters #\n",
    "## CHANGE VALUES HERE ##\n",
    "n_hidden_layers = [8]\n",
    "n_hidden_nodes = [16, 32, 64, 128, 256, 512]\n",
    "########################\n",
    "learning_rates = [1e-3]\n",
    "dropouts = [0.25]  \n",
    "batchnorm = [False]\n",
    "L2 = [0.0]\n",
    "batch_size = 512\n",
    "hyperparam_space = itertools.product(\n",
    "    n_hidden_layers, n_hidden_nodes, learning_rates, dropouts, batchnorm, L2)\n",
    "kf = KFold(n_splits=10)\n",
    "epochs = 300\n",
    "opt_val_score = None\n",
    "\n",
    "best_heuristic = lambda x, y: y['MSE'] < x['MSE']\n",
    "\n",
    "str1=[]\n",
    "str2=[]\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "for params in hyperparam_space:\n",
    "    print('\\nhidden layers: {}, nodes per layer: {}, learning rate: {}, dropout: {}, do batchnorm: {}, L2: {}\\t'.format(\n",
    "        params[0], params[1], params[2], params[3], params[4], params[5]))\n",
    "    # K-fold cross validation\n",
    "    train_losses_kf = []\n",
    "    val_scores_kf = {key: [] for key in test_metrics}\n",
    "    k = 0\n",
    "    for train_index, val_index in kf.split(x_train):\n",
    "        print(f'k-fold: {k}')\n",
    "        \n",
    "        # None adds a new axis\n",
    "        x_kftrain, y_kftrain = \\\n",
    "            x_train[train_index], y_train[train_index, None]\n",
    "        x_kfval, y_kfval = \\\n",
    "            x_train[val_index], y_train[val_index, None]\n",
    "\n",
    "        dataset_kftrain = gen_loaders(x_kftrain, y_kftrain, batch_size)\n",
    "        dataset_kfval = gen_loaders(x_kfval, y_kfval, batch_size)\n",
    "\n",
    "        model = MLP(\n",
    "            input_dim=x_train.shape[1],\n",
    "            layers=params[0],\n",
    "            nodes=params[1],\n",
    "            dropout=params[3],\n",
    "            do_batchnorm=params[4],\n",
    "            output_dim=1\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=params[2], weight_decay=params[5])\n",
    "\n",
    "        train_loss, val_scores = \\\n",
    "            train(model, criterion, dataset_kftrain, dataset_kfval,\n",
    "                  optimizer, epochs, device, test_metrics, best_heuristic)\n",
    "\n",
    "        train_losses_kf.append(train_loss)\n",
    "        for key, fun in test_metrics.items():\n",
    "            val_scores_kf[key].append(val_scores[key])\n",
    "        k += 1\n",
    "        \n",
    "    mean_train_loss = np.mean(train_losses_kf)\n",
    "    mean_val_scores = {key:\n",
    "                       np.mean(val_scores_kf[key]) for key in test_metrics}\n",
    "    std_val_scores = {key:\n",
    "                       np.std(val_scores_kf[key]) for key in test_metrics}\n",
    "\n",
    "    # for line plot\n",
    "    all_scores_layers[params[0]] = val_scores_kf\n",
    "    all_train_layers[params[0]] = train_losses_kf\n",
    "    all_scores_nodes[params[1]] = val_scores_kf\n",
    "    all_train_nodes[params[1]] = train_losses_kf\n",
    "\n",
    "\n",
    "    # output\n",
    "    print(f'R={mean_val_scores[\"pearsonr\"]} R std={std_val_scores[\"pearsonr\"]} MSE={mean_val_scores[\"MSE\"]} MSE std={std_val_scores[\"MSE\"]}')\n",
    "    print(\n",
    "        f'hyperparams.:{str(params)} with validation score={mean_val_scores[\"pearsonr\"]:12.5}')\n",
    "    str1.append(f'R={mean_val_scores[\"pearsonr\"]} R std={std_val_scores[\"pearsonr\"]} MSE={mean_val_scores[\"MSE\"]} MSE std={std_val_scores[\"MSE\"]}')\n",
    "    str2.append(f'hyperparams.:{str(params)} with validation score={mean_val_scores[\"pearsonr\"]:12.5}')\n",
    "\n",
    "    # best hyperparameters\n",
    "    if opt_val_score is None \\\n",
    "            or mean_val_scores['pearsonr'] > opt_val_score:\n",
    "        opt_val_score = mean_val_scores['pearsonr']\n",
    "        opt_params = params\n",
    "\n",
    "end_t = time.time()\n",
    "\n",
    "print(\n",
    "    f'best hyperparams.:{str(opt_params)} with validation score={opt_val_score:12.5} in {end_t-start_t}s')\n",
    "\n",
    "print()\n",
    "for i, s in enumerate(str1):\n",
    "    print(s)\n",
    "    print(str2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis on nodes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "data = []\n",
    "for nodes in all_scores_nodes:\n",
    "    data.append(all_scores_nodes[nodes]['pearsonr'])\n",
    "ax.boxplot(data, labels=[f'{k}' for k in all_scores_nodes.keys()])\n",
    "# ax.set_facecolor('aliceblue')\n",
    "ax.set_xlabel('nodes')\n",
    "ax.set_ylabel('R')\n",
    "plt.title('Pearson R score vs no. of nodes per hidden layer')\n",
    "plt.grid(color='lightgray')\n",
    "plt.savefig('mlp_R_vs_nodes.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "data = []\n",
    "for nodes in all_scores_nodes:\n",
    "    data.append(all_scores_nodes[nodes]['MSE'])\n",
    "ax.boxplot(data, labels=[f'{k}' for k in all_scores_nodes.keys()])\n",
    "# ax.set_facecolor('aliceblue')\n",
    "ax.set_xlabel('nodes')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.title('MSE vs no. of nodes per hidden layer')\n",
    "plt.grid(color='lightgray')\n",
    "plt.savefig('mlp_MSE_vs_nodes.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "analysis on layers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "data = []\n",
    "for nodes in all_scores_layers:\n",
    "    data.append(all_scores_layers[nodes]['pearsonr'])\n",
    "ax.boxplot(data, labels=[f'{k}' for k in all_scores_layers.keys()])\n",
    "# ax.set_facecolor('aliceblue')\n",
    "ax.set_xlabel('hidden layers')\n",
    "ax.set_ylabel('R')\n",
    "plt.title('Pearson R score vs no. of hidden layers')\n",
    "plt.grid(color='lightgray')\n",
    "plt.savefig('mlp_R_vs_layers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "data = []\n",
    "for nodes in all_scores_layers:\n",
    "    data.append(all_scores_layers[nodes]['MSE'])\n",
    "ax.boxplot(data, labels=[f'{k}' for k in all_scores_layers.keys()])\n",
    "# ax.set_facecolor('aliceblue')\n",
    "ax.set_xlabel('hidden layers')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.title('MSE vs no. of hidden layers')\n",
    "plt.grid(color='lightgray')\n",
    "plt.savefig('mlp_MSE_vs_layers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def save_data(nodes_tr, nodes_val, layers_tr, layers_val):\n",
    "    \"\"\"Save data to `mlp_overfit_data.pkl` in PKL format.\"\"\"\n",
    "    f = open(\"mlp_overfit_data.pkl\",\"wb\")\n",
    "    data = {'nodes_tr': nodes_tr, 'nodes_val': nodes_val, 'layers_tr':layers_tr, 'layers_val':layers_val}\n",
    "    pickle.dump(data,f)\n",
    "    f.close()\n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"Load data from `mlp_overfit_data.pkl`\"\"\"\n",
    "    data = pd.read_pickle('mlp_overfit_data.pkl')\n",
    "    nodes_tr, nodes_val, layers_tr, layers_val = data['nodes_tr'], data['nodes_val'], data['layers_tr'], data['layers_val']\n",
    "    return nodes_tr, nodes_val, layers_tr, layers_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "please use this two cells to save the data each time you perform a cross-validation run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(all_train_nodes, all_scores_nodes, all_train_layers, all_scores_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_tr, nodes_val, layers_tr, layers_val = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis on nodes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xscale('log', base=2)\n",
    "val_data = []\n",
    "nodes = []\n",
    "for n in nodes_val:\n",
    "    val_data.append(np.sqrt(nodes_val[n]['MSE']))\n",
    "    nodes.append(n)\n",
    "mval_data = np.mean(val_data, axis=1)\n",
    "ax.plot(nodes, mval_data, label='validation')\n",
    "stdval_data = np.std(val_data, axis=1)\n",
    "# ax.fill_between(nodes, mval_data-stdval_data, mval_data+stdval_data, alpha=0.2)\n",
    "ax.fill_between(nodes, \n",
    "                np.min(val_data, axis=1),\n",
    "                np.max(val_data, axis=1), \n",
    "                alpha=0.2)\n",
    "\n",
    "train_data = [np.sqrt(v) for v in nodes_tr.values()]\n",
    "mtrain_data = np.mean(train_data, axis=1)\n",
    "ax.plot(nodes, mtrain_data, label='training')\n",
    "stdtrain_data = np.std(val_data, axis=1)\n",
    "# ax.fill_between(nodes, mtrain_data-stdtrain_data, mtrain_data+stdtrain_data, alpha=0.2)\n",
    "ax.fill_between(nodes, \n",
    "                np.min(train_data, axis=1),\n",
    "                np.max(train_data, axis=1), \n",
    "                alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('nodes')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend()\n",
    "plt.title(f'RMSE loss vs no. of nodes per hidden layer. layers={n_hidden_layers[0]}')\n",
    "plt.grid(color='lightgray')\n",
    "\n",
    "plt.savefig(f'mlp_RMSE_vs_nodes_lineplot_{n_hidden_layers[0]}layers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "analysis on layers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "val_data = []\n",
    "layers = []\n",
    "for n in layers_val:\n",
    "    val_data.append( np.sqrt(layers_val[n]['MSE']) )\n",
    "    layers.append(n)\n",
    "mval_data = np.mean(val_data, axis=1)\n",
    "ax.plot(layers, mval_data, label='validation')\n",
    "stdval_data = np.std(val_data, axis=1)\n",
    "ax.fill_between(layers, mval_data-stdval_data, mval_data+stdval_data, alpha=0.2)\n",
    "\n",
    "train_data = [ np.sqrt(v) for v in layers_tr.values()]\n",
    "mtrain_data = np.mean(train_data, axis=1)\n",
    "ax.plot(layers, mtrain_data, label='training')\n",
    "stdtrain_data = np.std(val_data, axis=1)\n",
    "ax.fill_between(layers, mtrain_data-stdtrain_data, mtrain_data+stdtrain_data, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('hidden layers')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(loc='lower right')\n",
    "plt.title(f'RMSE loss vs no. of hidden layers. nodes={n_hidden_nodes[0]}')\n",
    "plt.grid(color='lightgray')\n",
    "\n",
    "plt.savefig('mlp_RMSE_vs_layers_lineplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot(111)\n",
    "val_data = []\n",
    "layers = []\n",
    "for n in layers_val:\n",
    "    val_data.append( np.sqrt(layers_val[n]['MSE']) )\n",
    "    layers.append(n)\n",
    "mval_data = np.mean(val_data, axis=1)\n",
    "ax.plot(layers, mval_data, label='validation')\n",
    "stdval_data = np.std(val_data, axis=1)\n",
    "ax.fill_between(layers, \n",
    "                np.min(val_data, axis=1),\n",
    "                np.max(val_data, axis=1), \n",
    "                alpha=0.2)\n",
    "\n",
    "train_data = [ np.sqrt(v) for v in layers_tr.values()]\n",
    "mtrain_data = np.mean(train_data, axis=1)\n",
    "ax.plot(layers, mtrain_data, label='training')\n",
    "stdtrain_data = np.std(val_data, axis=1)\n",
    "ax.fill_between(layers, \n",
    "                np.min(train_data, axis=1),\n",
    "                np.max(train_data, axis=1), \n",
    "                alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('hidden layers')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(loc='lower right')\n",
    "plt.title(f'RMSE loss vs no. of hidden layers. nodes={n_hidden_nodes[0]}')\n",
    "plt.grid(color='lightgray')\n",
    "\n",
    "plt.savefig(f'mlp_RMSE_vs_layers_lineplot_{n_hidden_nodes[0]}nodes.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HydraNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(skempi_csv, sep=';')\n",
    "# df = df.iloc[:4096, :]\n",
    "\n",
    "# filter duplicated\n",
    "df_test = df_test[~df_test.duplicated(subset=[\"#Pdb\", \"Mutation(s)_cleaned\"])]\n",
    "\n",
    "# remove without target\n",
    "df_test = df_test.dropna(subset=['Affinity_mut_parsed'])\n",
    "df_test = df_test.dropna(subset=['Affinity_wt_parsed'])\n",
    "df_test = df_test.dropna(subset=['Temperature'])\n",
    "\n",
    "pdb_n = 0\n",
    "name_wt = df_test.iloc[pdb_n,0]\n",
    "\n",
    "test_dmat = np.load(wt_features_path + 'D_mat/' + name_wt + '.npy')\n",
    "\n",
    "test_lj = np.load(wt_features_path + 'U_LJ/' + name_wt + '.npy')\n",
    "\n",
    "test_el = np.load(wt_features_path + 'U_el/' + name_wt + '.npy')\n",
    "\n",
    "print('u_dmat sample ranges: ', test_dmat.min(), '\\t', test_dmat.max())\n",
    "print('u_lj sample ranges: ', test_lj.min(), '\\t', test_lj.max())\n",
    "print('u_el sample ranges: ', test_el.min(), '\\t', test_el.max())\n",
    "# test_el.min()\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fig.add_subplot(221)\n",
    "plt.hist(np.log(test_lj.flatten() - test_lj.min() + 1))\n",
    "plt.title('log normalized lj potentials')\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plt.hist(test_el.flatten())\n",
    "plt.title('electrostatic potentials')\n",
    "\n",
    "fig.add_subplot(223)\n",
    "plt.hist(test_dmat.flatten())\n",
    "plt.title('pairwise distances')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_wt = '1A22_A_B'\n",
    "name_mut = '1A22_A_B_NB31A'\n",
    "\n",
    "def standardize(arr):\n",
    "    return (arr - np.mean(arr)) / np.std(arr)\n",
    "\n",
    "def normalize_range(arr):\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "def log_standardize(arr):\n",
    "    temp = np.log(arr - arr.min() + 1)\n",
    "    return standardize(temp)\n",
    "\n",
    "def log_normalize(arr):\n",
    "    temp = np.log(arr - arr.min() + 1)\n",
    "    return normalize(temp)\n",
    "\n",
    "wt_arr = np.load(wt_features_path + name_wt + '.npy')\n",
    "\n",
    "wt_arr[0] = log_standardize(np.clip(wt_arr[0], None, 1e12))\n",
    "wt_arr[1] = standardize(wt_arr[1])\n",
    "wt_arr[2] = standardize(wt_arr[2])\n",
    "\n",
    "print('n entries outside of [0,1] range: ', np.sum(np.logical_or(wt_arr < 0, wt_arr > 1)))\n",
    "\n",
    "# wt_arr[0] = normalize_range(wt_arr[0])\n",
    "# wt_arr[1] = normalize_range(wt_arr[1])\n",
    "# wt_arr[2] = normalize_range(wt_arr[2])\n",
    "\n",
    "mut_arr = np.load(mut_features_path + name_mut + '.npy')\n",
    "\n",
    "mut_arr[0] = log_standardize(np.clip(mut_arr[0], None, 1e12))\n",
    "mut_arr[1] = standardize(mut_arr[1])\n",
    "mut_arr[2] = standardize(mut_arr[2])\n",
    "\n",
    "\n",
    "\n",
    "# mut_arr[0] = normalize_range(mut_arr[0])\n",
    "# mut_arr[1] = normalize_range(mut_arr[1])\n",
    "# mut_arr[2] = normalize_range(mut_arr[2])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5, 12))\n",
    "\n",
    "fig.add_subplot(221)\n",
    "plt.imshow(wt_arr.transpose(1, 2, 0))\n",
    "plt.title(f'{name_wt}')\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plt.imshow(mut_arr.transpose(1, 2, 0))\n",
    "plt.title(f'{name_mut}')\n",
    "\n",
    "plt.tight_layout()\n",
    "# wt_arr.shape\n",
    "print('wt max: ', wt_arr.max(), '\\t mut max: ', mut_arr.max(),\n",
    "      'wt min: ', wt_arr.min(), '\\t mut min: ', mut_arr.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
