{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47ff722cfd1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwt_features_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_features_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskempi_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ..scripts.constants import wt_features_path, mut_features_path, skempi_csv, R\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we are in a conda virtual env\n",
    "try:\n",
    "    os.environ[\"CONDA_DEFAULT_ENV\"]\n",
    "except KeyError:\n",
    "    print(\"\\tPlease init the conda environment!\\n\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "def standardize(arr):\n",
    "    return (arr - np.mean(arr)) / np.std(arr)\n",
    "\n",
    "\n",
    "def siamese_preprocessing(pandas_row):\n",
    "    name_wt = pandas_row[1].iloc[0]\n",
    "    name_mut = pandas_row[1].iloc[0] + '_' + pandas_row[1].iloc[2].replace(',', '_')\n",
    "\n",
    "    if not Path(mut_features_path + 'D_mat/' + name_mut + '.npy').exists():\n",
    "        print(f'ERROR: {name_mut} does not exist.', '\\n')\n",
    "        return None\n",
    "    if not Path(wt_features_path + 'D_mat/' + name_wt + '.npy').exists():\n",
    "        print(f'ERROR: {name_wt} does not exist.', '\\n')\n",
    "        return None\n",
    "\n",
    "    d_mat_wt = standardize(np.load(wt_features_path + 'D_mat/' + name_wt + '.npy'))\n",
    "    u_lj_wt = standardize(np.load(wt_features_path + 'U_LJ/' + name_wt + '.npy'))\n",
    "    u_el_wt = standardize(np.load(wt_features_path + 'U_el/' + name_wt + '.npy'))\n",
    "\n",
    "    wt_arr = np.stack([d_mat_wt, u_lj_wt, u_el_wt])\n",
    "\n",
    "    d_mat_mut = standardize(np.load(mut_features_path + 'D_mat/' + name_mut + '.npy'))\n",
    "    u_lj_mut = standardize(np.load(mut_features_path + 'U_LJ/' + name_mut + '.npy'))\n",
    "    u_el_mut = standardize(np.load(mut_features_path + 'U_el/' + name_mut + '.npy'))\n",
    "\n",
    "    mut_arr = np.stack([d_mat_mut, u_lj_mut, u_el_mut])\n",
    "\n",
    "    # calculate DDG\n",
    "    A_wt = pandas_row[1]['Affinity_wt_parsed']\n",
    "    A_mut = pandas_row[1]['Affinity_mut_parsed']\n",
    "\n",
    "    # print(pandas_row[1]['Temperature'])\n",
    "    temp = float(re.match(\"[0-9]*\", pandas_row[1]['Temperature'])[0])\n",
    "    if math.isnan(temp):\n",
    "        raise ValueError('temperature should not be NaN.')\n",
    "\n",
    "    DG_wt = R * temp * np.log(A_wt)\n",
    "    DG_mut = R * temp * np.log(A_mut)\n",
    "    DDG = DG_mut - DG_wt\n",
    "\n",
    "    # debug print\n",
    "    print(f'parsed {name_mut}')\n",
    "\n",
    "    return np.stack([wt_arr, mut_arr]), DDG\n",
    "\n",
    "def gen_loaders(x, y, batch_size):\n",
    "    x_tensor, y_tensor = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "    data = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "    loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "class HydraNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # feature map output: [(W - K + 2P) / S] + 1\n",
    "        # include batch norm, dropout (remember model.train() and model.eval() !!!)\n",
    "        self.cnn1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2),\n",
    "                                  # output: (256-3)/2 + 1 =\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3, stride=2),\n",
    "                                  nn.BatchNorm2d(8),\n",
    "\n",
    "                                  nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3, stride=2),\n",
    "                                  nn.BatchNorm2d(16),\n",
    "                                  #nn.AvgPool2d()\n",
    "\n",
    "                                  nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3, stride=2),\n",
    "                                  nn.BatchNorm2d(32),\n",
    "\n",
    "                                  nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(2)\n",
    "                                  )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2),\n",
    "                                  # output: (256-3)/2 + 1 =\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3, stride=2),\n",
    "                                  nn.BatchNorm2d(8),\n",
    "\n",
    "                                  nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3, stride=2),\n",
    "                                  nn.BatchNorm2d(16),\n",
    "                                  #nn.AvgPool2d()  \n",
    "\n",
    "                                  nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3, stride=2),\n",
    "                                  nn.BatchNorm2d(32),\n",
    "\n",
    "                                  nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(2)\n",
    "                                  )\n",
    "\n",
    "        # each output of self.cnn will have dimension 1024, so when concatenated we have 2048\n",
    "        self.fc = nn.Sequential(  # nn.Linear(2048, 512),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            nn.Linear(2 * 32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            nn.Linear(16, 1))\n",
    "\n",
    "    def forward(self, x1):\n",
    "        output1 = self.cnn1(x1[:, 0])\n",
    "        #output1 = torch.mean(output1.view(output1.size(0), output1.size(1), -1), dim=2)\n",
    "        output2 = output1.view(output1.size()[0], -1)\n",
    "\n",
    "        output3 = self.cnn2(x1[:, 1])\n",
    "        #output3 = torch.mean(output3.view(output3.size(0), output3.size(1), -1), dim=2)\n",
    "        output4 = output3.view(output3.size()[0], -1)\n",
    "\n",
    "        output5 = torch.cat((output2, output4), 1)\n",
    "\n",
    "        return self.fc(output5)\n",
    "\n",
    "\n",
    "def train(_model, _criterion, dataset_train, dataset_test, _optimizer, n_epochs):\n",
    "    print(\"Starting training\")\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train an epoch\n",
    "        _model.train()\n",
    "        train_losses = []\n",
    "        for batch_x, batch_y in dataset_train:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            logits = _model.forward(batch_x)\n",
    "            loss = _criterion(logits, batch_y)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Compute the gradient\n",
    "            _optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model with a gradient step\n",
    "            _optimizer.step()\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Test the quality on the test set\n",
    "        _model.eval()\n",
    "        mse_test = []\n",
    "        for batch_x, batch_y in dataset_test:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            # TODO: solve allocation error in GPU\n",
    "            prediction = _model.forward(batch_x)\n",
    "            mse_test.append(_criterion(prediction, batch_y).item())\n",
    "            # ^^ could be source of memory leak\n",
    "        print(\n",
    "            \"Epoch {} | Train loss: {:.5f} Test loss: {:.5f}\".format(epoch, train_loss, sum(mse_test) / len(mse_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv(skempi_csv, sep=';')\n",
    "    # df = df.iloc[:4096, :]\n",
    "\n",
    "    # filter duplicated\n",
    "    df = df[~df.duplicated(subset=[\"#Pdb\", \"Mutation(s)_cleaned\"])]\n",
    "\n",
    "    # remove without target\n",
    "    df = df.dropna(subset=['Affinity_mut_parsed'])\n",
    "    df = df.dropna(subset=['Affinity_wt_parsed'])\n",
    "    df = df.dropna(subset=['Temperature'])\n",
    "\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "\n",
    "    n_non_existant = 0\n",
    "    for data in mp.Pool(5).imap_unordered(siamese_preprocessing, df.iterrows()):\n",
    "        if data is None:\n",
    "            n_non_existant += 1\n",
    "        else:\n",
    "            input_list.append(data[0])\n",
    "            target_list.append(data[1])\n",
    "\n",
    "    print(f'{n_non_existant} PDBs do not have features.')\n",
    "\n",
    "    input_arr = np.array(input_list).astype(np.float32)\n",
    "    target_arr = np.array(target_list).astype(np.float32)[..., np.newaxis]\n",
    "\n",
    "    x_tr, x_te, y_tr, y_te = train_test_split(input_arr, target_arr, test_size=0.2, random_state=42)\n",
    "    print('x_tr: ', x_tr.shape, 'x_te: ', x_te.shape, 'y_tr: ', y_tr.shape, 'y_te: ', y_te.shape, '\\n')\n",
    "    # x_tr, x_val, y_tr, y_val = train_test_split(x_tr, y_tr, test_size=0.25, random_state=42)\n",
    "\n",
    "    train_data = gen_loaders(x_tr, y_tr, 16)\n",
    "    # val_data = gen_loaders(x_val, y_val, 16)\n",
    "    test_data = gen_loaders(x_te, y_te, 16)\n",
    "\n",
    "    # model = HydraNet()\n",
    "    num_epochs = 100\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # If a GPU is available\n",
    "    if not torch.cuda.is_available():\n",
    "        print('WARNING: using CPU.')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train the logistic regression model with the Adam optimizer\n",
    "    criterion = torch.nn.MSELoss()  # MSE loss for regression\n",
    "    model_hydra = HydraNet().to(device)\n",
    "\n",
    "    # perhaps add larger degree of weight decay\n",
    "    optimizer = torch.optim.Adam(model_hydra.parameters(), lr=learning_rate, weight_decay=1e-1)\n",
    "    # note that below validation data should actually be used...\n",
    "    train(model_hydra, criterion, train_data, test_data, optimizer, num_epochs)\n",
    "\n",
    "    model_hydra.eval().to(\"cpu\")\n",
    "    pred = model_hydra.forward(torch.from_numpy(x_te))\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "    R = pearsonr(y_te.squeeze(), pred.squeeze())[0]\n",
    "    print(f'Pearson R score: {R:.5}', '\\n')\n",
    "    print(f'Test RMSE: {torch.sqrt(criterion(torch.tensor(pred), torch.tensor(y_te)))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
