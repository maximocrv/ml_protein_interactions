{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='../data/'\n",
    "SKEMPI_CSV=DATA_PATH + 'skempi_v2_cleaned.csv'\n",
    "df = pd.read_csv(SKEMPI_CSV, sep=';')\n",
    "df.head()\n",
    "name_wt  = df.iloc[1, :][0]\n",
    "name_mut = df.iloc[1, :][0] + '_' + df.iloc[1, :][2].replace(',', '_')\n",
    "# print(name_wt, '\\t', name_mut)\n",
    "\n",
    "WT_FEATURE_PATH=DATA_PATH + 'openmm/'\n",
    "MUT_FEATURE_PATH=DATA_PATH + 'openmm_mutated/'\n",
    "MLP_OUTPUT_PATH=DATA_PATH + 'mlp_features.csv'\n",
    "d_mat_wt  = np.load(WT_FEATURE_PATH +'D_mat/'+name_wt +'.npy')\n",
    "u_lj_wt   = np.load(WT_FEATURE_PATH +'U_LJ/'+name_wt +'.npy')\n",
    "u_el_wt   = np.load(WT_FEATURE_PATH +'U_el/'+name_wt +'.npy')\n",
    "test = np.stack([d_mat_wt, u_lj_wt, u_el_wt])\n",
    "test_ii = np.stack([test, test])\n",
    "# test_ii.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 1CSE_E_I_LI38S \n",
      "\n",
      "parsed 1CSE_E_I_LI38G \n",
      "\n",
      "parsed 1CSE_E_I_LI38D \n",
      "\n",
      "parsed 1CSE_E_I_LI38I \n",
      "\n",
      "parsed 1CSE_E_I_LI38P \n",
      "\n",
      "parsed 1CSE_E_I_LI38E \n",
      "\n",
      "parsed 1ACB_E_I_LI38P \n",
      "\n",
      "parsed 1ACB_E_I_LI38G \n",
      "\n",
      "parsed 1ACB_E_I_LI38I \n",
      "\n",
      "parsed 1ACB_E_I_LI38D \n",
      "\n",
      "parsed 1ACB_E_I_LI38S \n",
      "\n",
      "parsed 1SBN_E_I_RI38K \n",
      "\n",
      "parsed 1ACB_E_I_LI38E \n",
      "\n",
      "parsed 1SIB_E_I_KI46R \n",
      "\n",
      "parsed 1TM1_E_I_YI42A \n",
      "\n",
      "parsed 1TM1_E_I_RI46A \n",
      "\n",
      "parsed 1TM1_E_I_YI42G \n",
      "\n",
      "parsed 1TM1_E_I_RI48A \n",
      "\n",
      "parsed 1TM1_E_I_RI48A_RI46A \n",
      "\n",
      "parsed 1TM1_E_I_TI39A \n",
      "\n",
      "parsed 1TM1_E_I_TI39D \n",
      "\n",
      "parsed 1TM1_E_I_RI48C \n",
      "\n",
      "parsed 1TM1_E_I_EI41A \n",
      "\n",
      "parsed 1Y33_E_I_PI39T \n",
      "\n",
      "parsed 1Y34_E_I_AI41E \n",
      "\n",
      "parsed 1TM1_E_I_TI39D_EI41A \n",
      "\n",
      "parsed 1TM1_E_I_VI51A \n",
      "\n",
      "parsed 1TM1_E_I_TI39A_EI41A \n",
      "\n",
      "parsed 1Y1K_E_I_AI39T \n",
      "\n",
      "parsed 1Y3C_E_I_AI43R \n",
      "\n",
      "parsed 1TM1_E_I_TI39A \n",
      "\n",
      "parsed 1Y4A_E_I_SI40E_RI39M \n",
      "\n",
      "parsed 1TM1_E_I_EI41A \n",
      "\n",
      "parsed 1Y3D_E_I_AI48R \n",
      "\n",
      "parsed 1TM1_E_I_TI39P \n",
      "\n",
      "parsed 1Y3B_E_I_SI41E \n",
      "\n",
      "parsed 1TM1_E_I_RI46A \n",
      "\n",
      "parsed 1TM1_E_I_RI48A \n",
      "\n",
      "parsed 1Y48_E_I_AI46R \n",
      "\n",
      "parsed 1TM1_E_I_RI43A \n",
      "\n",
      "parsed 1TM1_E_I_EI41S \n",
      "\n",
      "parsed 1TM4_E_I_GI40M \n",
      "\n",
      "parsed 1TM1_E_I_EI41S_MI40R \n",
      "\n",
      "parsed 1TM3_E_I_KI40M \n",
      "\n",
      "parsed 1TO1_E_I_AI42Y \n",
      "\n",
      "parsed 1TM7_E_I_YI40M \n",
      "\n",
      "parsed 1TM5_E_I_AI40M \n",
      "\n",
      "parsed 1TMG_E_I_FI40M \n",
      "\n",
      "parsed 1TM1_E_I_MI40A \n",
      "\n",
      "parsed 1TM1_E_I_MI40G \n",
      "\n",
      "parsed 1TM1_E_I_YI42A \n",
      "\n",
      "parsed 1TM1_E_I_MI40K \n",
      "\n",
      "parsed 1TM1_E_I_MI40Y \n",
      "\n",
      "parsed 2SIC_E_I_MI67K \n",
      "\n",
      "parsed 2SIC_E_I_MI67E \n",
      "\n",
      "parsed 2SIC_E_I_MI67R \n",
      "\n",
      "parsed 2SIC_E_I_MI67G \n",
      "parsed 2SIC_E_I_MI67A \n",
      "parsed 1TM1_E_I_MI40F\n",
      "\n",
      " \n",
      "\n",
      "parsed 2SIC_E_I_MI67D \n",
      "\n",
      "parsed 2SIC_E_I_MI67H \n",
      "\n",
      "parsed 1IAR_A_B_IA5A \n",
      "\n",
      "parsed 1IAR_A_B_IA5R \n",
      "\n",
      "parsed 2SIC_E_I_MI67L \n",
      "\n",
      "parsed 1IAR_A_B_TA6A \n",
      "\n",
      "parsed 1IAR_A_B_TA6D \n",
      "\n",
      "parsed 2SIC_E_I_MI67I \n",
      "\n",
      "ERROR: 1IAR_A_B_EA9K does not exist. \n",
      "parsed 2SIC_E_I_MI67V\n",
      "ERROR: 1IAR_A_B_EA9A does not exist. \n",
      "\n",
      " \n",
      "\n",
      "parsed 1IAR_A_B_EA9Q \n",
      "\n",
      "parsed 1IAR_A_B_IA11A \n",
      "\n",
      "parsed 1IAR_A_B_KA12S \n",
      "\n",
      "parsed 1IAR_A_B_QA8R \n",
      "\n",
      "parsed 1IAR_A_B_QA8A \n",
      "\n",
      "parsed 1IAR_A_B_TA13A \n",
      "\n",
      "parsed 1IAR_A_B_KA12E \n",
      "\n",
      "parsed 1IAR_A_B_TA13D \n",
      "\n",
      "parsed 1IAR_A_B_EA19A \n",
      "\n",
      "parsed 1IAR_A_B_SA16A \n",
      "\n",
      "parsed 1IAR_A_B_NA15D \n",
      "parsed 1IAR_A_B_SA16D \n",
      "\n",
      "\n",
      "parsed 1IAR_A_B_NA15A \n",
      "\n",
      "parsed 1IAR_A_B_RA53Q \n",
      "\n",
      "parsed 1IAR_A_B_KA77E \n",
      "\n",
      "parsed 1IAR_A_B_RA81A \n",
      "\n",
      "parsed 1IAR_A_B_QA78A \n",
      "\n",
      "parsed 1IAR_A_B_QA78E \n",
      "\n",
      "parsed 1IAR_A_B_KA77A \n",
      "\n",
      "parsed 1IAR_A_B_EA19R \n",
      "\n",
      "parsed 1IAR_A_B_FA82A \n",
      "\n",
      "parsed 1IAR_A_B_KA84A \n",
      "\n",
      "parsed 1IAR_A_B_KA84D \n",
      "\n",
      "parsed 1IAR_A_B_RA88Q \n",
      "\n",
      "ERROR: 1IAR_A_B_RA88D does not exist. \n",
      "\n",
      "parsed 1IAR_A_B_RA81E \n",
      "\n",
      "parsed 1IAR_A_B_FA82D \n",
      "parsed 1IAR_A_B_RA85E \n",
      "\n",
      "\n",
      "ERROR: 1IAR_A_B_NA89R does not exist. \n",
      "\n",
      "parsed 1IAR_A_B_RA85A \n",
      "\n",
      "parsed 1IAR_A_B_WA91D \n",
      "\n",
      "parsed 1BRS_A_D_KA25A \n",
      "\n",
      "parsed 1BRS_A_D_RA57A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q \n",
      "\n",
      "parsed 1IAR_A_B_WA91A \n",
      "\n",
      "parsed 1IAR_A_B_NA89A \n",
      "\n",
      "parsed 1IAR_A_B_RA88A \n",
      "\n",
      "parsed 1BRS_A_D_YD29A \n",
      "\n",
      "parsed 1BRS_A_D_RA85A \n",
      "\n",
      "parsed 1BRS_A_D_YD29F \n",
      "\n",
      "parsed 1BRS_A_D_WD38F \n",
      "\n",
      "parsed 1BRS_A_D_HA100A \n",
      "\n",
      "parsed 1BRS_A_D_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_TD42A \n",
      "\n",
      "parsed 1BRS_A_D_WD44F \n",
      "\n",
      "parsed 1BRS_A_D_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_ED78A \n",
      "\n",
      "parsed 1BRS_A_D_KA25A_YD29A \n",
      "\n",
      "parsed 1BRS_A_D_KA25A_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_KA25A_TD42A \n",
      "\n",
      "parsed 1BRS_A_D_KA25A_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_KA25A_WD38F \n",
      "\n",
      "parsed 1BRS_A_D_KA25A_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_KA25A_ED78A \n",
      "\n",
      "parsed 1BRS_A_D_RA57A_YD29A \n",
      "\n",
      "parsed 1BRS_A_D_RA57A_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_RA57A_WD38F \n",
      "\n",
      "parsed 1BRS_A_D_RA57A_TD42A \n",
      "\n",
      "parsed 1BRS_A_D_RA57A_ED78A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q_WD38F \n",
      "\n",
      "parsed 1BRS_A_D_RA57A_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q_ED78A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q_YD29A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q_TD42A \n",
      "\n",
      "parsed 1BRS_A_D_RA85A_TD42A \n",
      "\n",
      "parsed 1BRS_A_D_RA81Q_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_RA85A_ED78A \n",
      "\n",
      "parsed 1BRS_A_D_RA85A_YD29A \n",
      "\n",
      "parsed 1BRS_A_D_RA85A_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_RA85A_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_RA85A_WD38F \n",
      "\n",
      "parsed 1BRS_A_D_HA100A_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_HA100A_YD29A \n",
      "\n",
      "parsed 1BRS_A_D_HA100A_ED78A \n",
      "\n",
      "parsed 1BRS_A_D_HA100A_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_HA100A_WD38F \n",
      "\n",
      "parsed 1BRS_A_D_HA100A_TD42A \n",
      "\n",
      "parsed 1BRS_A_D_EA71Q \n",
      "parsed 1BRS_A_D_EA71Q_DD39A \n",
      "\n",
      "\n",
      "parsed 1BRS_A_D_EA71Q_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_EA71W_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_EA71W_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_HA100A_YD29F \n",
      "\n",
      "parsed 1BRS_A_D_RA57A_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_EA71F \n",
      "\n",
      "parsed 1BRS_A_D_EA71Y \n",
      "\n",
      "parsed 1BRS_A_D_EA71F_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_EA71S \n",
      "\n",
      "parsed 1BRS_A_D_EA71W \n",
      "\n",
      "parsed 1BRS_A_D_EA71W_ED74A \n",
      "\n",
      "parsed 1BRS_A_D_EA71C \n",
      "\n",
      "parsed 1BRS_A_D_EA71A_DD39A \n",
      "\n",
      "parsed 1BRS_A_D_EA71F_DD39Aparsed 1BRS_A_D_RA57A \n",
      "\n",
      " \n",
      "\n",
      "parsed 1BRS_A_D_RA57K \n",
      "\n",
      "parsed 1BRS_A_D_EA71A \n",
      "\n",
      "parsed 1BRS_A_D_HA100A \n",
      "\n",
      "parsed 1BRS_A_D_HA100G \n",
      "\n",
      "parsed 1BRS_A_D_EA71A_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_HA100D \n",
      "\n",
      "parsed 1BRS_A_D_HA100Q \n",
      "\n",
      "parsed 1BRS_A_D_HA100L \n",
      "\n",
      "parsed 1BRS_A_D_HA100Q_RA57K \n",
      "\n",
      "parsed 1B2U_A_D_AA27K \n",
      "\n",
      "parsed 1BRS_A_D_HA100Q_RA57A \n",
      "\n",
      "parsed 1B2U_A_D_AA27K_AD36D \n",
      "\n",
      "parsed 1B2U_A_D_AD36D \n",
      "\n",
      "parsed 1B3S_A_D_FD30Y \n",
      "\n",
      "parsed 1B3S_A_D_AA102H \n",
      "\n",
      "parsed 1BRS_A_D_DD35A \n",
      "\n",
      "parsed 1BRS_A_D_DD39A \n",
      "\n",
      "parsed 1B3S_A_D_AA102H_FD30Y \n",
      "parsed 1BRS_A_D_ED74A\n",
      " \n",
      "\n",
      "parsed 1BRS_A_D_ED78A \n",
      "\n",
      "parsed 1B2S_A_D_AA27K \n",
      "\n",
      "parsed 1BRS_A_D_KA25A \n",
      "\n",
      "parsed 1B2S_A_D_AD43T \n",
      "\n",
      "parsed 1BRS_A_D_WA33F \n",
      "\n",
      "parsed 1B2S_A_D_AA27K_AD43T \n",
      "\n",
      "parsed 1BRS_A_D_RA57A \n",
      "\n",
      "parsed 1X1X_A_D_AD76E \n",
      "parsed 1BRS_A_D_EA58A\n",
      " \n",
      "\n",
      "parsed 1BRS_A_D_EA71Aparsed 1BRS_A_D_RA85A parsed 1BRS_A_D_NA56A \n",
      "\n",
      "\n",
      "\n",
      "parsed 1BRS_A_D_DA52A \n",
      "\n",
      " parsed 1BRS_A_D_HA100Aparsed 1X1W_A_D_AD81Eparsed 1BRS_A_D_KA25A  \n",
      "\n",
      "parsed 1BRS_A_D_WA33F\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "parsed 1BRS_A_D_DA52A\n",
      "\n",
      "\n",
      "parsed 1BRS_A_D_NA56A \n",
      " \n",
      "\n",
      "\n",
      "parsed 1BRS_A_D_RA57A parsed 1BRS_A_D_EA58A \n",
      "\n",
      "parsed 1BRS_A_D_EA71Aparsed 1BRS_A_D_HA100A \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "ERROR: 1SBB_A_B_LB20T_VB26Y_YB91V does not exist. \n",
      "\n",
      "ERROR: 1JCK_A_B_NB23A does not exist. \n",
      "\n",
      "parsed 1SBB_A_B_LB20T \n",
      "\n",
      "parsed 1JCK_A_B_NB60A \n",
      "\n",
      "parsed 1SBB_A_B_VB26Y \n",
      "\n",
      "parsed 1JCK_A_B_TB20A \n",
      "\n",
      "parsed 1SBB_A_B_YB91V \n",
      "\n",
      "parsed 1JCK_A_B_YB26A \n",
      "\n",
      "ERROR: 1JCK_A_B_QB210A does not exist. \n",
      "\n",
      "parsed 1JCK_A_B_YB90Aparsed 1SBB_A_B_LB20T  \n",
      "\n",
      "\n",
      "\n",
      "parsed 1SBB_A_B_VB26Y \n",
      "\n",
      "parsed 1SBB_A_B_YB91V \n",
      "ERROR: 1SBB_A_B_LB20T_VB26Y_YB91V does not exist. \n",
      "\n",
      "\n",
      "ERROR: 1JCK_A_B_NB23A does not exist. parsed 1JCK_A_B_VB91A \n",
      "\n",
      "parsed 1JCK_A_B_TB20A \n",
      "\n",
      "\n",
      "\n",
      "parsed 1JCK_A_B_FB176Aparsed 1JCK_A_B_YB26Aparsed 1JCK_A_B_NB60A   \n",
      "parsed 1JCK_A_B_YB90Aparsed 1JCK_A_B_KB103A \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "parsed 1JCK_A_B_VB91A\n",
      "\n",
      " parsed 1JCK_A_B_KB103A\n",
      "\n",
      " \n",
      "ERROR: 1JCK_A_B_QB210A does not exist. parsed 1JCK_A_B_FB176Aparsed 1JCK_A_B_GB102A\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "parsed 1GCQ_AB_C_PC5A \n",
      "\n",
      "parsed 1GCQ_AB_C_AC42G \n",
      "\n",
      "parsed 1GCQ_AB_C_PC18A \n",
      "\n",
      "parsed 1GCQ_AB_C_GC21V \n",
      "\n",
      "parsed 1GCQ_AB_C_PC19A \n",
      "\n",
      "parsed 1GCQ_AB_C_PC67A \n",
      "\n",
      "parsed 1GCQ_AB_C_WC47Y \n",
      "\n",
      "parsed 1E96_A_B_KA132E \n",
      "\n",
      "parsed 1E96_A_B_DA38N \n",
      "\n",
      "parsed 1AK4_A_D_PD85A \n",
      "\n",
      "parsed 1E96_A_B_MA45T \n",
      "\n",
      "parsed 1E96_A_B_NA26H \n",
      "\n",
      "parsed 1E96_A_B_LA134R \n",
      "\n",
      "parsed 1E96_A_B_IA33N \n",
      "\n",
      "parsed 1AK4_A_D_VD86A \n",
      "\n",
      "ERROR: 1AK4_A_D_AD88G does not exist. \n",
      "\n",
      "parsed 1AK4_A_D_AD88V \n",
      "\n",
      "parsed 1AK4_A_D_HD87R \n",
      "\n",
      "parsed 1AK4_A_D_PD90A \n",
      "\n",
      "parsed 1AK4_A_D_HD87Aparsed 1AK4_A_D_HD87Q \n",
      "\n",
      " \n",
      "\n",
      "parsed 1AK4_A_D_GD89A \n",
      "\n",
      "ERROR: 1AK4_A_D_AD92G does not exist.parsed 1AK4_A_D_GD89V  \n",
      "\n",
      "\n",
      "\n",
      "parsed 1AK4_A_D_ID91V \n",
      "\n",
      "parsed 1AK4_A_D_AD92V \n",
      "\n",
      "parsed 1AK4_A_D_LD83T_VD86P_HD87A_AD88M_ID91L_AD92P_MD96I \n",
      "\n",
      "parsed 1M9E_A_D_AD76H \n",
      "\n",
      "parsed 1AK4_A_D_ID91A \n",
      "\n",
      "parsed 1AK4_A_D_PD93A \n",
      "\n",
      "parsed 2B42_A_B_HA357Q \n",
      "\n",
      "parsed 2B42_A_B_HA357K parsed 2I26_N_L_AN29V \n",
      "parsed 1AK4_A_D_PD90V \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ERROR: 3BT1_A_U_RU135A does not exist. \n",
      "parsed 2I26_N_L_SN60R \n",
      "\n",
      "ERROR: 3BT1_A_U_KU137A does not exist. parsed 2B42_A_B_HA357A \n",
      "\n",
      "\n",
      "ERROR: 3BT1_A_U_RU140A does not exist. \n",
      "\n",
      "\n",
      "ERROR: 3BT1_A_U_HU141A does not exist.\n",
      "ERROR: 3BT1_A_U_RU143A does not exist.  \n",
      "\n",
      "\n",
      "\n",
      "parsed 2HLE_A_B_LA87R \n",
      "\n",
      "parsed 2HLE_A_B_KA141Q \n",
      "\n",
      "parsed 1AHW_AB_C_IC141Q_TC143I_YC145T \n",
      "\n",
      "parsed 1AHW_AB_C_KC154A_KC155A \n",
      "\n",
      "parsed 1AHW_AB_C_YC146A \n",
      "\n",
      "parsed 1AHW_AB_C_YC145Aparsed 1AHW_AB_C_TC110A_KC111A  \n",
      "\n",
      "\n",
      "\n",
      "parsed 1AHW_AB_C_KC138A_DC139A \n",
      "\n",
      "parsed 1AHW_AB_C_TC156A \n",
      "\n",
      "parsed 1AHW_AB_C_KC158A_NC160A_NC162A \n",
      "\n",
      "parsed 1AHW_AB_C_LC165A \n",
      "\n",
      "parsed 1AHW_AB_C_TC186A \n",
      "\n",
      "parsed 1AHW_AB_C_TC159A \n",
      "\n",
      "parsed 1AHW_AB_C_NC188A \n",
      "\n",
      "parsed 1AHW_AB_C_RC189A_KC190A \n",
      "\n",
      "parsed 1AHW_AB_C_SC184A_RC185A \n",
      "\n",
      "parsed 1AHW_AB_C_VC187A \n",
      "\n",
      "parsed 1AHW_AB_C_DC167A \n",
      "\n",
      "parsed 1UUZ_A_D_HA62Q \n",
      "\n",
      "parsed 1UUZ_A_D_CA64A \n",
      "\n",
      "parsed 1UUZ_A_D_HA62A \n",
      "\n",
      "parsed 1UUZ_A_D_HA62D parsed 1UUZ_A_D_HA62N\n",
      " \n",
      "\n",
      "\n",
      "parsed 1DVF_AB_CD_HA30A \n",
      "\n",
      "parsed 1DVF_AB_CD_YA32A \n",
      "\n",
      "parsed 1DVF_AB_CD_SA93A \n",
      "\n",
      "parsed 1DVF_AB_CD_YA50A \n",
      "\n",
      "parsed 1DVF_AB_CD_WA92A \n",
      "\n",
      "parsed 1DVF_AB_CD_WB52A \n",
      "\n",
      "parsed 1DVF_AB_CD_YA49A \n",
      "\n",
      "parsed 1DVF_AB_CD_TB30A \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 1DVF_AB_CD_DB54A \n",
      "\n",
      "parsed 1DVF_AB_CD_RB99A \n",
      "\n",
      "parsed 1DVF_AB_CD_EB98A \n",
      "\n",
      "ERROR: 1DVF_AB_CD_YB101A does not exist. \n",
      "\n",
      "parsed 1DVF_AB_CD_DB100A \n",
      "\n",
      "parsed 1DVF_AB_CD_YB101F \n",
      "parsed 1DVF_AB_CD_EB98A\n",
      " parsed 1DVF_AB_CD_YB32A \n",
      "parsed 1DVF_AB_CD_DB54A\n",
      "\n",
      " \n",
      "parsed 1DVF_AB_CD_NB56A\n",
      "\n",
      " parsed 1DVF_AB_CD_DB58Aparsed 1DVF_AB_CD_YA49A\n",
      "\n",
      "parsed 1DVF_AB_CD_YA32Aparsed 1DVF_AB_CD_DB58A   \n",
      "parsed 1DVF_AB_CD_NB56A\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "parsed 1DVF_AB_CD_WB52Aparsed 1DVF_AB_CD_DB100A\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "parsed 1DVF_AB_CD_ND55A \n",
      "\n",
      "parsed 1DVF_AB_CD_YC49A \n",
      "\n",
      "parsed 1DVF_AB_CD_HD33A \n",
      "\n",
      "parsed 1DVF_AB_CD_YD102A \n",
      "\n",
      "parsed 1DVF_AB_CD_QD104A \n",
      "\n",
      "parsed 1DVF_AB_CD_DD52A \n",
      "\n",
      "parsed 1DVF_AB_CD_RD106A \n",
      "\n",
      "parsed 1DVF_AB_CD_KD30A \n",
      "\n",
      "parsed 1DVF_AB_CD_ID101A \n",
      "\n",
      "parsed 1DVF_AB_CD_YA32A_RD106A \n",
      "\n",
      "parsed 1DVF_AB_CD_NB56A_QD104A \n",
      "\n",
      "parsed 1DVF_AB_CD_EB98A_YD102A \n",
      "\n",
      "parsed 1DVF_AB_CD_DB58A_QD104A \n",
      "\n",
      "parsed 1DVF_AB_CD_YA49A_ND55A \n",
      "\n",
      "parsed 1DVF_AB_CD_DB54A_YC49A \n",
      "\n",
      "parsed 1DVF_AB_CD_DB100A_HD33A \n",
      "\n",
      "parsed 1DVF_AB_CD_YA49A_HD33A \n",
      "\n",
      "parsed 1DVF_AB_CD_DB100A_DD52A \n",
      "\n",
      "parsed 1DVF_AB_CD_DB100A_ND55A \n",
      "\n",
      "parsed 1DVF_AB_CD_NB56A_ND55A \n",
      "\n",
      "parsed 1DVF_AB_CD_NB56A_HD33A \n",
      "\n",
      "parsed 1DVF_AB_CD_WB52A_QD104A \n",
      "\n",
      "parsed 1VFB_AB_C_YA32A \n",
      "\n",
      "parsed 1VFB_AB_C_HA30A \n",
      "\n",
      "parsed 1VFB_AB_C_YA49A \n",
      "\n",
      "parsed 1VFB_AB_C_SA93A \n",
      "\n",
      "parsed 1VFB_AB_C_WA92A \n",
      "\n",
      "parsed 1VFB_AB_C_TB30A \n",
      "\n",
      "parsed 1VFB_AB_C_DB54A \n",
      "\n",
      "parsed 1VFB_AB_C_WB52A \n",
      "\n",
      "parsed 1VFB_AB_C_YA50A \n",
      "\n",
      "parsed 1VFB_AB_C_YB32A \n",
      "\n",
      "parsed 1VFB_AB_C_EB98A \n",
      "\n",
      "ERROR: 1VFB_AB_C_YB101A does not exist. \n",
      "\n",
      "ERROR: 1VFB_AB_C_YB101F does not exist. \n",
      "\n",
      "parsed 1VFB_AB_C_DB58A \n",
      "\n",
      "parsed 1VFB_AB_C_RB99A \n",
      "\n",
      "parsed 1VFB_AB_C_NB56A \n",
      "\n",
      "parsed 1VFB_AB_C_DB100A\n",
      " \n",
      "parsed 1VFB_AB_C_SB28N \n",
      "\n",
      "parsed 1VFB_AB_C_SB28D \n",
      "\n",
      "ERROR: 1VFB_AB_C_GB31W does not exist. \n",
      "\n",
      "parsed 1VFB_AB_C_SB28Q \n",
      "\n",
      "parsed 1VFB_AB_C_GB31A \n",
      "\n",
      "parsed 1VFB_AB_C_YB32E \n",
      "parsed 1VFB_AB_C_GB31E\n",
      " \n",
      "\n",
      "ERROR: 1VFB_AB_C_RB99Y does not exist. ERROR: 1VFB_AB_C_RB99W does not exist.\n",
      "\n",
      " \n",
      "parsed 1VFB_AB_C_DB58E\n",
      "ERROR: 1VFB_AB_C_NA31W does not exist.ERROR: 1VFB_AB_C_YA32W does not exist.   \n",
      "\n",
      "\n",
      "parsed 1VFB_AB_C_SB28Eparsed 1VFB_AB_C_DB58N\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "parsed 1VFB_AB_C_LA46D \n",
      "\n",
      "\n",
      "\n",
      "ERROR: 1VFB_AB_C_TA52F does not exist. \n",
      "\n",
      "parsed 1EFN_A_B_TA13H_EA14H \n",
      "\n",
      "parsed 1VFB_AB_C_TA53R \n",
      "\n",
      "parsed 1EFN_A_B_TA13H \n",
      "\n",
      "parsed 1VFB_AB_C_LA46E \n",
      "\n",
      "parsed 1VFB_AB_C_YA50R \n",
      "parsed 1VFB_AB_C_YA50K\n",
      " \n",
      "\n",
      "parsed 2GOX_A_B_NB38A \n",
      "\n",
      "parsed 2GOX_A_B_RB31A parsed 2GOX_A_B_RB31A\n",
      " \n",
      "\n",
      "\n",
      "parsed 3D5S_A_C_AC31R \n",
      "\n",
      "parsed 2GOX_A_B_NB38A \n",
      "parsed 1EFN_A_B_IA12A \n",
      "\n",
      "\n",
      "parsed 3D5S_A_C_AC31R \n",
      "\n",
      "parsed 2NOJ_A_B_RB24A_NB31A \n",
      "\n",
      "parsed 3D5R_A_C_AC38Nparsed 3D5R_A_C_AC38N  \n",
      "\n",
      "\n",
      "\n",
      "parsed 3BP8_A_C_FA115A \n",
      "\n",
      "parsed 3BP8_A_C_AC51F \n",
      "\n",
      "ERROR: 2HRK_A_B_TB55R does not exist. \n",
      "\n",
      "parsed 3BP8_A_C_FA115A_AC51F \n",
      "\n",
      "ERROR: 2HRK_A_B_RB100A does not exist. parsed 2VIS_AB_C_IC89T \n",
      "\n",
      "\n",
      "ERROR: 2HRK_A_B_YB104A does not exist.\n",
      " ERROR: 2HRK_A_B_TA108R does not exist.\n",
      "\n",
      " \n",
      "\n",
      "parsed 2VIR_AB_C_TC89I \n",
      "\n",
      "ERROR: 2HRK_A_B_RA147A does not exist. \n",
      "\n",
      "parsed 2VIR_AB_C_SC115L \n",
      "\n",
      "parsed 2WPT_A_B_NA31V \n",
      "\n",
      "parsed 2WPT_A_B_DA30L parsed 2HRK_A_B_TB55V \n",
      "\n",
      "\n",
      "\n",
      "parsed 2HRK_A_B_KA140A \n",
      "\n",
      "parsed 2WPT_A_B_RA35T \n",
      "\n",
      "parsed 2WPT_A_B_NA31V_RA35T \n",
      "\n",
      "parsed 2WPT_A_B_EA36H \n",
      "\n",
      "parsed 2WPT_A_B_RA39E \n",
      "\n",
      "parsed 2WPT_A_B_EA27A \n",
      "\n",
      "parsed 2WPT_A_B_DA30L_NA31V_RA35T \n",
      "\n",
      "parsed 2HRK_A_B_TA108V \n",
      "\n",
      "parsed 2WPT_A_B_NA31V_RA35T_EA36H_RA39E \n",
      "\n",
      "parsed 2WPT_A_B_NA31V_RA35T_RA39E \n",
      "\n",
      "parsed 2WPT_A_B_NA31A \n",
      "\n",
      "parsed 2WPT_A_B_DA30A \n",
      "\n",
      "parsed 2WPT_A_B_VA34A \n",
      "\n",
      "parsed 2WPT_A_B_EA38A \n",
      "\n",
      "parsed 2WPT_A_B_RA39A \n",
      "\n",
      "parsed 2WPT_A_B_DA48A \n",
      "\n",
      "parsed 2WPT_A_B_SA47A \n",
      "\n",
      "parsed 2WPT_A_B_RA35A \n",
      "\n",
      "parsed 2WPT_A_B_FB79A_DA30A \n",
      "\n",
      "parsed 2WPT_A_B_FB79A_VA34A \n",
      "\n",
      "ERROR: 2WPT_A_B_FB79A_YA51A does not exist. \n",
      "parsed 2WPT_A_B_FB79A_NA31A \n",
      "\n",
      "\n",
      "ERROR: 2WPT_A_B_FB79A_YA52A does not exist. parsed 2WPT_A_B_PA53A \n",
      "\n",
      "\n",
      "\n",
      "parsed 2WPT_A_B_NB65A \n",
      "\n",
      "parsed 2WPT_A_B_YA51A \n",
      "\n",
      "parsed 2WPT_A_B_SB67A \n",
      "\n",
      "parsed 2WPT_A_B_SB71A \n",
      "\n",
      "parsed 2WPT_A_B_YA52A \n",
      "\n",
      "parsed 2WPT_A_B_SB70A \n",
      "\n",
      "parsed 2WPT_A_B_NB68A \n",
      "\n",
      "parsed 2WPT_A_B_RB47A \n",
      "\n",
      "parsed 2WPT_A_B_KB90A \n",
      "\n",
      "parsed 2WPT_A_B_TB80A \n",
      "\n",
      "parsed 2WPT_A_B_RB47Aparsed 2WPT_A_B_SB77A  \n",
      "\n",
      "\n",
      "\n",
      "parsed 2WPT_A_B_FB79A \n",
      "\n",
      "parsed 2WPT_A_B_NB65A parsed 2WPT_A_B_QB85Aparsed 2WPT_A_B_SB67A \n",
      " \n",
      "parsed 2WPT_A_B_NB68A\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "parsed 2WPT_A_B_SB70Aparsed 2WPT_A_B_VB91A  \n",
      "\n",
      "parsed 2WPT_A_B_SB77A\n",
      "parsed 2WPT_A_B_SB71A  \n",
      "\n",
      "parsed 2WPT_A_B_FB79A\n",
      "parsed 2WPT_A_B_TB80A\n",
      "  \n",
      "\n",
      "\n",
      "parsed 2WPT_A_B_QB85A\n",
      "parsed 2WPT_A_B_KB90A parsed 2WPT_A_B_VB91A\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "parsed 1EMV_A_B_QA15E_LA16F_TA18K_TA19K_NA22R_DA24E_TA25G_SA26A_SA27T_EA30D_LA31D_VA32N_TA36R_HA37E_EA40R_MA41L_SA46D_KA55R_EA56D_GA57D_DA58R_DA59E_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 2VLP_A_B_AB54R \n",
      "\n",
      "parsed 2WPT_A_B_EA14Q_FA15L_KA17T_KA18T_RA21N_EA23D_GA24T_AA25S_TA26S_DA29E_DA30L_NA31V_RA35T_EA36H_RA39E_LA40M_DA45S_RA54K_DA55E_DA56G_RA57D_EA58D_EA62S_KA66N_EA67T_IA68V_EA70Q \n",
      "\n",
      "parsed 2VLQ_A_B_AB86F \n",
      "\n",
      "parsed 2VLN_A_B_AB75N \n",
      "\n",
      "parsed 1EMV_A_B_EA30D_LA31D_VA32N_TA36R_HA37E_EA40R_MA41L_SA46D_KA55R_EA56D_GA57D_DA58R_DA59E_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 2WPT_A_B_DA29E_DA30L_NA31V_RA35T_EA36H_RA39E_LA40M_DA45S_RA54K_DA55E_DA56G_RA57D_EA58D \n",
      "\n",
      "parsed 2VLO_A_B_AB95K \n",
      "\n",
      "parsed 2WPT_A_B_EA14Q_FA15L_KA17T_KA18T_RA21N_EA23D_GA24T_AA25S_TA26S \n",
      "\n",
      "parsed 1EMV_A_B_QA15E_LA16F_TA18K_TA19K_NA22R_SA46D_KA55R_EA56D_GA57D_DA58R_DA59E_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 1EMV_A_B_QA15E_LA16F_TA18K_TA19K_NA22R_DA24E_TA25G_SA26A_SA27T_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 2WPT_A_B_EA23D_GA24T_AA25S_TA26S_DA29E_DA30L_NA31V_RA35T_EA36H_RA39E_LA40M \n",
      "\n",
      "parsed 1EMV_A_B_QA15E_LA16F_SA46D_KA55R_EA56D_GA57D_DA58R_DA59E_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 1EMV_A_B_SA46D_KA55R_EA56D_GA57D_DA58R_DA59E_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 1EMV_A_B_QA15E_LA16F_TA18K_TA19K_NA22R_DA24E_TA25G_SA26A_SA27T_SA46D_KA55R_EA56D_GA57D_DA58R_DA59E_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 2WPT_A_B_KA17T_KA18T_RA21N_EA23D_GA24T_AA25S_TA26S_DA29E_DA30L_NA31V_RA35T_EA36H_RA39E_LA40M \n",
      "\n",
      "parsed 2WPT_A_B_DA29E_DA30L_NA31V_RA35T_EA36H_RA39E_LA40M \n",
      "\n",
      "parsed 2WPT_A_B_EA14Q_FA15L_KA17T_KA18T_RA21N_EA23D_GA24T_AA25S_TA26S_DA29E_DA30L_NA31V_RA35T_EA36H_RA39E_LA40M \n",
      "\n",
      "parsed 1EMV_A_B_NB72A \n",
      "\n",
      "parsed 2WPT_A_B_EA14Q_FA15L_KA17T_KA18T_RA21N_EA23D_GA24T_AA25S_TA26S_DA29E_DA30L_NA31V_RA35T_EA36H_RA39E_LA40M_DA45S_RA54K_DA55E_DA56G_RA57D_EA58D \n",
      "\n",
      "parsed 1EMV_A_B_SB77A \n",
      "\n",
      "parsed 1EMV_A_B_RB54A \n",
      "\n",
      "parsed 1EMV_A_B_SA63E_NA67K_TA68E_VA69I_QA71E \n",
      "\n",
      "parsed 1EMV_A_B_NB75A \n",
      "parsed 1EMV_A_B_SB74A\n",
      " \n",
      "\n",
      "parsed 1EMV_A_B_SB78A \n",
      "\n",
      "parsed 1EMV_A_B_SB84A \n",
      "\n",
      "parsed 1EMV_A_B_VB98A \n",
      "\n",
      "parsed 1EMV_A_B_FB86A \n",
      "\n",
      "parsed 1EMV_A_B_TB87A \n",
      "\n",
      "parsed 1EMV_A_B_QB92A \n",
      "\n",
      "parsed 1EMV_A_B_FB86A_LA31A \n",
      "\n",
      "parsed 1EMV_A_B_FB86A_VA32A \n",
      "\n",
      "parsed 1EMV_A_B_FB86A_VA35A \n",
      "\n",
      "parsed 1EMV_A_B_KB97A \n",
      "\n",
      "parsed 1EMV_A_B_DA24A \n",
      "\n",
      "parsed 1EMV_A_B_FB86A_YA52A \n",
      "\n",
      "parsed 1EMV_A_B_CA21A \n",
      "\n",
      "parsed 1EMV_A_B_FB86A_YA53A \n",
      "\n",
      "parsed 1EMV_A_B_NA22A \n",
      "\n",
      "33 PDBs do not have features.\n"
     ]
    }
   ],
   "source": [
    "# check if we are in a conda virtual env\n",
    "try:\n",
    "   os.environ[\"CONDA_DEFAULT_ENV\"]\n",
    "except KeyError:\n",
    "   print(\"\\tPlease init the conda environment!\\n\")\n",
    "   exit(1)\n",
    "\n",
    "def standardize(arr):\n",
    "    return (arr - np.mean(arr)) / np.std(arr)\n",
    "\n",
    "DATA_PATH='../data/'\n",
    "SKEMPI_CSV=DATA_PATH + 'skempi_v2_cleaned.csv'\n",
    "WT_FEATURE_PATH=DATA_PATH + 'openmm/'\n",
    "MUT_FEATURE_PATH=DATA_PATH + 'openmm_mutated/'\n",
    "MLP_OUTPUT_PATH=DATA_PATH + 'mlp_features.csv'\n",
    "\n",
    "R = (8.314/4184)  # kcal mol^-1 K^-1\n",
    "\n",
    "def siamese_preprocessing(pandas_row):\n",
    "    name_wt  = pandas_row[1].iloc[0]\n",
    "    name_mut = pandas_row[1].iloc[0] + '_' + pandas_row[1].iloc[2].replace(',', '_')\n",
    "\n",
    "    # matrix_features = ['D_mat', 'U_LJ', 'U_el']\n",
    "    # for feature in matrix_features:\n",
    "    #     pass\n",
    "\n",
    "    if not Path(MUT_FEATURE_PATH + 'D_mat/' + name_mut + '.npy').exists():\n",
    "        print(f'ERROR: {name_mut} does not exist.', '\\n')\n",
    "        return None\n",
    "    if not Path(WT_FEATURE_PATH + 'D_mat/' + name_wt + '.npy').exists():\n",
    "        print(f'ERROR: {name_wt} does not exist.', '\\n')\n",
    "        return None\n",
    "\n",
    "    d_mat_wt  = standardize(np.load(WT_FEATURE_PATH + 'D_mat/' + name_wt +'.npy'))\n",
    "    u_lj_wt   = standardize(np.load(WT_FEATURE_PATH + 'U_LJ/' + name_wt +'.npy'))\n",
    "    u_el_wt   = standardize(np.load(WT_FEATURE_PATH + 'U_el/' + name_wt +'.npy'))\n",
    "    \n",
    "    wt_arr = np.stack([d_mat_wt, u_lj_wt, u_el_wt])\n",
    "    \n",
    "    d_mat_mut = standardize(np.load(MUT_FEATURE_PATH + 'D_mat/' + name_mut + '.npy'))\n",
    "    u_lj_mut  = standardize(np.load(MUT_FEATURE_PATH + 'U_LJ/' + name_mut + '.npy'))\n",
    "    u_el_mut  = standardize(np.load(MUT_FEATURE_PATH + 'U_el/' + name_mut + '.npy'))\n",
    "    \n",
    "    mut_arr = np.stack([d_mat_mut, u_lj_mut, u_el_mut])\n",
    "    \n",
    "    # calculate DDG\n",
    "    A_wt  = pandas_row[1]['Affinity_wt_parsed']\n",
    "    A_mut = pandas_row[1]['Affinity_mut_parsed']\n",
    "\n",
    "    temp = float(re.match(\"[0-9]*\", pandas_row[1]['Temperature'])[0])\n",
    "    if math.isnan(temp):\n",
    "        raise ValueError('temperature should not be NaN.')\n",
    "    \n",
    "    DG_wt = R * temp * np.log(A_wt)\n",
    "    DG_mut = R * temp * np.log(A_mut)\n",
    "    DDG = DG_mut - DG_wt\n",
    "    \n",
    "    # debug print\n",
    "    print(f'parsed {name_mut}', '\\n')\n",
    "\n",
    "    return (np.stack([wt_arr, mut_arr]), DDG)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv(SKEMPI_CSV, sep=';')\n",
    "    df = df.iloc[:500, :]\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    n_non_existant = 0\n",
    "    for data in mp.Pool(5).imap_unordered(siamese_preprocessing, df.iterrows()):\n",
    "        if data is None:\n",
    "            n_non_existant += 1\n",
    "        else:\n",
    "            input_list.append(data[0])\n",
    "            target_list.append(data[1])\n",
    "\n",
    "    print(f'{n_non_existant} PDBs do not have features.')\n",
    "#     print(df_out)\n",
    "#     df_out.to_csv(MLP_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_data(n_samples, channels, nx, ny):\n",
    "    data = np.random.randn(n_samples, 2, channels, nx, ny).astype(np.float32)\n",
    "    outputs = np.random.uniform(0, 100, n_samples).astype(np.float32)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, outputs, test_size=0.2, random_state=1)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "    \n",
    "def gen_loaders(x, y, batch_size):    \n",
    "    x_tensor, y_tensor = torch.from_numpy(x), torch.from_numpy(y)\n",
    "    \n",
    "    data = TensorDataset(x_tensor, y_tensor)\n",
    "    \n",
    "    loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr = np.array(input_list).astype(np.float32)\n",
    "target_arr = np.array(target_list).astype(np.float32)[...,np.newaxis]\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(input_arr, target_arr, test_size=0.2, random_state=1)\n",
    "train_data = gen_loaders(x_tr, y_tr, 16)\n",
    "test_data = gen_loaders(x_te, y_te, 16)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = gen_random_data(1000, 3, 256, 256)\n",
    "\n",
    "# train_set, test_set = gen_loaders(x_train, x_test, y_train, y_test, 100)\n",
    "\n",
    "# print('train_batches:')\n",
    "# for batch_x_tr, batch_y_tr in train_set:\n",
    "#     print(batch_x_tr[:,0].shape, batch_x_tr[:,1].shape, batch_y_tr.shape)\n",
    "\n",
    "# print('\\n', 'test_batches:')\n",
    "# for batch_x_te, batch_y_te in test_set:\n",
    "#     print(batch_x_te[:,0].shape, batch_x_te[:,1].shape, batch_y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydraNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # feature map output: [(W - K + 2P) / S] + 1\n",
    "        # include batch norm, dropout (remember model.train() and model.eval() !!!)\n",
    "        self.cnn1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2), #output: (256-3)/2 + 1 = \n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3, stride=2),\n",
    "                                 \n",
    "                                 nn.Conv2d(in_channels=8, out_channels=64, kernel_size=3, stride=2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3, stride=2),\n",
    "                                 \n",
    "                                 nn.Conv2d(in_channels=64, out_channels=512, kernel_size=3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3, stride=2),\n",
    "                                 nn.Dropout2d(p=0.5),\n",
    "                                 \n",
    "                                 nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2)\n",
    "                                )\n",
    "        \n",
    "        self.cnn2 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2), #output: (256-3)/2 + 1 = \n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3, stride=2),\n",
    "                                 \n",
    "                                 nn.Conv2d(in_channels=8, out_channels=64, kernel_size=3, stride=2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3, stride=2),\n",
    "                                 \n",
    "                                 nn.Conv2d(in_channels=64, out_channels=512, kernel_size=3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3, stride=2),\n",
    "                                 nn.Dropout2d(p=0.5),\n",
    "                                 \n",
    "                                 nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2)\n",
    "                                )\n",
    "        \n",
    "        # each output of self.cnn will have dimension 1024, so when concatenated we have 2048\n",
    "        self.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512, 128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(128, 64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(64, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 1))\n",
    "        \n",
    "    def forward(self, x1):\n",
    "        output1 = self.cnn1(x1[:, 0])\n",
    "        output2 = output1.view(output1.size()[0], -1)\n",
    "        \n",
    "        output3 = self.cnn2(x1[:, 1])\n",
    "        output4 = output3.view(output3.size()[0], -1)\n",
    "        \n",
    "        output5 = torch.cat((output2, output4), 1)\n",
    "        \n",
    "        return self.fc(output5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HydraNet().to(\"cuda\")\n",
    "\n",
    "# for x_batch, _ in test:\n",
    "#     out = model.forward(x_batch)\n",
    "#     print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    @param model: torch.nn.Module\n",
    "    @param criterion: torch.nn.modules.loss._Loss\n",
    "    @param dataset_train: torch.utils.data.DataLoader\n",
    "    @param dataset_test: torch.utils.data.DataLoader\n",
    "    @param optimizer: torch.optim.Optimizer\n",
    "    @param num_epochs: int\n",
    "    \"\"\"\n",
    "    print(\"Starting training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train an epoch\n",
    "        model.train()\n",
    "        for batch_x, batch_y in dataset_train:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            # TODO: insert your code here\n",
    "            logits = model.forward(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            # Compute the gradient\n",
    "            # TODO: insert your code here\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model with a gradient step\n",
    "            # TODO: insert your code here\n",
    "            optimizer.step()\n",
    "\n",
    "        # Test the quality on the test set\n",
    "        model.eval()\n",
    "        mse_test = []\n",
    "        for batch_x, batch_y in dataset_test:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            prediction = model(batch_x)\n",
    "            mse_test.append(criterion(prediction, batch_y).item())\n",
    "\n",
    "        print(\"Epoch {} | Test loss: {:.5f}\".format(epoch, sum(mse_test)/len(mse_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 0 | Test loss: 8.17737\n",
      "Epoch 1 | Test loss: 8.50894\n",
      "Epoch 2 | Test loss: 7.10878\n",
      "Epoch 3 | Test loss: 6.82305\n",
      "Epoch 4 | Test loss: 6.42736\n",
      "Epoch 5 | Test loss: 7.43152\n",
      "Epoch 6 | Test loss: 6.78779\n",
      "Epoch 7 | Test loss: 6.65716\n",
      "Epoch 8 | Test loss: 6.57612\n",
      "Epoch 9 | Test loss: 7.04030\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# If a GPU is available (should be on Colab, we will use it)\n",
    "if not torch.cuda.is_available():\n",
    "  raise Exception(\"Things will go much quicker if you enable a GPU in Colab under 'Runtime / Change Runtime Type'\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the logistic regression model with the Adam optimizer\n",
    "criterion = torch.nn.MSELoss() # MSE loss for regression\n",
    "model_hydra = HydraNet().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_hydra.parameters(), lr=learning_rate)\n",
    "train(model_hydra, criterion, train_data, test_data, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson R score: 0.55534 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_hydra(torch.from_numpy(x_test).cuda())\n",
    "from scipy.stats import pearsonr\n",
    "model_hydra.eval()\n",
    "\n",
    "\n",
    "pred = model_hydra(torch.from_numpy(x_te).to(device))\n",
    "pred = pred.cpu().detach().numpy()\n",
    "R = pearsonr(y_te.squeeze(), pred.squeeze())[0]\n",
    "print(f'Pearson R score: {R:.5}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6405)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(criterion(torch.tensor(pred), torch.tensor(y_te)))\n",
    "# criterion(pred.squeeze(), y_te.squeeze()).item()\n",
    "# print(f'Test RMSE: {np.sqrt(criterion(pred.squeeze(), y_te.squeeze()).item())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
